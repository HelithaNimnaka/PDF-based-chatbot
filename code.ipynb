{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "# Load DeepSeek Model via Hugging Face Hub\n",
    "repo_id = \"deepseek-ai/DeepSeek-V3\"\n",
    "client = HuggingFaceHub(\n",
    "    repo_id=repo_id, \n",
    "    model_kwargs={\"temperature\": 0.5, \"top_k\": 10}, \n",
    "    huggingfacehub_api_token = #add your token here\n",
    ")\n",
    "\n",
    "# Load the PDF\n",
    "def load_pdf(pdf_path):\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    documents = loader.load()\n",
    "    return \"\\n\".join([doc.page_content for doc in documents])\n",
    "\n",
    "# Split text into chunks\n",
    "def split_text(text, chunk_size=500, overlap=50):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=overlap\n",
    "    )\n",
    "    return text_splitter.split_text(text)\n",
    "\n",
    "# Function to embed text using a pre-trained sentence transformer\n",
    "def embed_text(chunks):\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # Small & fast embedding model\n",
    "    embeddings = model.encode(chunks, convert_to_tensor=True)\n",
    "    return embeddings, model\n",
    "\n",
    "# Create and save FAISS Index\n",
    "def build_faiss_index(embeddings, index_path=\"faiss_index\"):\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings.cpu().detach().numpy())  # Convert to NumPy array\n",
    "    faiss.write_index(index, index_path)  # Save index\n",
    "    return index\n",
    "\n",
    "# Load FAISS Index if available\n",
    "def load_faiss_index(file_path=\"faiss_index\"):\n",
    "    return faiss.read_index(file_path)\n",
    "\n",
    "# Search for relevant chunks using FAISS\n",
    "import numpy as np\n",
    "\n",
    "def retrieve_relevant_chunks(query, index, text_chunks, embed_model, top_k=3, min_length=30, max_words=100, similarity_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Retrieves relevant text chunks while filtering out irrelevant or short content.\n",
    "    \"\"\"\n",
    "    if index.ntotal == 0:\n",
    "        return [\"No relevant context found.\"]\n",
    "\n",
    "    # Get query embedding\n",
    "    query_embedding = embed_model.encode([query], convert_to_tensor=True).cpu().detach().numpy().astype(np.float32)\n",
    "\n",
    "    # FAISS search\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "\n",
    "    relevant_chunks = []\n",
    "    for i, distance in zip(indices[0], distances[0]):\n",
    "        if i < len(text_chunks):  # Ensure valid index\n",
    "            chunk = text_chunks[i].strip()\n",
    "\n",
    "            # Check similarity (Lower distance = Higher similarity)\n",
    "            similarity_score = 1 / (1 + distance)  # Normalize to [0,1]\n",
    "\n",
    "            if similarity_score >= similarity_threshold and len(chunk) > min_length:\n",
    "                # Limit chunk to `max_words`\n",
    "                words = chunk.split()[:max_words]\n",
    "                relevant_chunks.append(\" \".join(words))\n",
    "\n",
    "    return relevant_chunks if relevant_chunks else [\"No relevant context found.\"]\n",
    "\n",
    "\n",
    "\n",
    "# Generate answer using DeepSeek-LLM\n",
    "def generate_answer(question, index, text_chunks, embed_model):\n",
    "    generic_questions = [\"hello\", \"hi\", \"hey\", \"how are you?\", \"what's up?\", \"good morning\", \"good evening\"]\n",
    "\n",
    "    # Handle generic greetings separately\n",
    "    if question.lower().strip() in generic_questions:\n",
    "        return \"Hello! How can I assist you today?\"\n",
    "\n",
    "    # Retrieve relevant text\n",
    "    relevant_chunks = retrieve_relevant_chunks(question, index, text_chunks, embed_model, top_k=2)\n",
    "\n",
    "    # If no meaningful context is found, prevent junk responses\n",
    "    if not relevant_chunks or \"No relevant context found.\" in relevant_chunks:\n",
    "        return \"I don't have relevant information in the document to answer this question.\"\n",
    "\n",
    "    # Limit context to only the most relevant sections\n",
    "    context = \"\\n\".join(relevant_chunks[:2])  # Take top 2 relevant chunks\n",
    "\n",
    "    # Construct refined prompt\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI assistant. Use the provided context to answer the question concisely and accurately.\n",
    "    \n",
    "    Context: \n",
    "    {context}\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    Answer:\"\"\"\n",
    "    \n",
    "    # Generate response\n",
    "    response = client.invoke(prompt)\n",
    "    \n",
    "    return response.split(\"Answer:\")[-1].strip() # Remove trailing spaces\n",
    "\n",
    "\n",
    "\n",
    "# File path to the PDF\n",
    "pdf_path = #add your pdf path here\n",
    "\n",
    "# Load and process the PDF\n",
    "text = load_pdf(pdf_path)\n",
    "text_chunks = split_text(text)\n",
    "\n",
    "# Check if FAISS index exists; if not, create it\n",
    "index_path = \"faiss_index\"\n",
    "if os.path.exists(index_path):\n",
    "    faiss_index = load_faiss_index(index_path)\n",
    "    embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # Ensure embed_model is available\n",
    "else:\n",
    "    embeddings, embed_model = embed_text(text_chunks)\n",
    "    faiss_index = build_faiss_index(embeddings, index_path)\n",
    "\n",
    "\n",
    "# Define chatbot function for Gradio\n",
    "def chatbot(query, history=None):  # Added history argument to prevent TypeError\n",
    "    return generate_answer(query, faiss_index, text_chunks, embed_model)\n",
    "\n",
    "# Create Gradio Chatbot Interface\n",
    "interface = gr.ChatInterface(\n",
    "    chatbot,\n",
    "    title=\"PDF-based Chatbot with DeepSeek\",\n",
    "    description=\"Ask questions based on the provided PDF book. The chatbot will only answer from the book's content.\",\n",
    ")\n",
    "\n",
    "# Launch the chatbot\n",
    "interface.launch(share=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
